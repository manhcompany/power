etl = {
  datasetA = {
    input = {
      path = "/Users/manhtran/Documents/code/power/spark/src/test/resources/input/input.csv"
      format = "csv"
      options = [
        {key = "header", value = true}
      ]
    }
    actions = [
      {
        operator = SELECT
        exprs = ["id as ID", "age as AGE", "act_name as act_name"]
      }
      {
        operator = DROPNULL
        columns = [
          """ID"""
        ]
      }
      {
        operator = FILTER
        exprs = [
          """ID = 1 or ID = 2"""
        ]
      }
      {
        operator = REPARTITION
        partitions = 1
      }
      {
        operator = AS_TEMP_TABLE
        table-name = """ABC"""
      }
    ]
  }

  except = {
    input = {
      load = "datasetA"
    }
    actions = [
      {
        operator = EXCEPT
        options = [
          {key = "OTHER_DATASETS", value = "datasetA"}
        ]
      }
    ]
    output = {
      path = "/Users/manhtran/Documents/code/power/spark/src/test/resources/output/except"
      format = "csv"
      mode = "overwrite"
      options = [
        {key = "header", value = true}
      ]
    }
  }

  films = {
    input = {
      path = "/Users/manhtran/Documents/code/power/spark/src/test/resources/input/films.csv"
      format = "csv"
      options = [
        {key = "header", value = true}
      ]
    }
    actions = [
      {
        operator = AS_TEMP_TABLE
        table-name = """films"""
      }
      {
        operator = SELECT
        exprs = ["""act_id as id""", """name"""]
      }
    ]
  }

  join_2 = {
    input = {
      load = "datasetA"
    }
    actions = [
      {
        operator = JOIN
        exprs = ["id"]
        join-type = "inner"
        options = [
          {key = "OTHER_DATASETS", value = "films"}
        ]
      }
    ]
    output = {
      path = "/Users/manhtran/Documents/code/power/spark/src/test/resources/output/join_2"
      format = "csv"
      mode = "overwrite"
      options = [
        {key = "header", value = true}
      ]
    }
  }

  join = {
    actions = [
      {
        operator = SQL
        sql = "select films.name as film_name, ABC.act_name as act_name from films inner join ABC on films.act_id = ABC.ID"
        options = [
          {key = "OTHER_DATASETS", value = "datasetA"}
          {key = "OTHER_DATASETS", value = "films"}
        ]
      }
      {
        operator = RENAME
        columns = [
          """ film_name >> FILM_NAME_XXX """
        ]
      }
      {
        operator = DEDUPLICATE
        colums = [
          """ FILM_NAME_XXX """
        ]
      }
      {
        operator = REPARTITION
        partitions = 1
      }
    ]
    output = {
      path = "/Users/manhtran/Documents/code/power/spark/src/test/resources/output/join"
      format = "csv"
      mode = "overwrite"
      options = [
        {key = "header", value = true}
      ]
    }
  }

  max = {
    actions = [
      {
        operator = SQL
        sql = """select max(ID) as MAX from ABC"""
        options = [
          {key = "OTHER_DATASETS", value = "datasetA"}
        ]
      }
    ]
  }

  us = {
    input = {
      path = "/Users/manhtran/Documents/code/power/spark/src/test/resources/input/us.csv"
      format = "csv"
      options = [
        {key = "header", value = true}
      ]
    }
  }

  facet = {
    input = {
      load = "datasetA"
    }
    actions = [
      {
        operator = FACET
        columns = ["ID", "AGE"]
        options = [
          {key = "date", value = "20190101"}
          {key = "dataset", value = "datasetA"}
        ]
      }
      {
        operator = REPARTITION
        partitions = 1
      }
    ]
    output = {
      path = "/Users/manhtran/Documents/code/power/spark/src/test/resources/output/facet"
      format = "csv"
      mode = "overwrite"
      options = [
        {key = "header", value = true}
      ]
    }
  }

  desc = {
    input = {
      load = "datasetA"
    }
    actions = [
      {
        operator = DESC
        options = [
          {key = "date", value = "20190101"}
          {key = "dataset", value = "datasetA"}
        ]
        describes = [
          {col = "AGE", summary = ["count"]}
          {col = "ID", summary = ["count"]}
        ]
      }
      {
        operator = SELECT
        exprs = [
          """cast(time_stamp as long) as time_stamp"""
          """cast(substr(date_time, 1, 4) as int) as year"""
          """cast(substr(date_time, 5, 2) as int) as month"""
          """cast(substr(date_time, 7, 2) as int) as day"""
          """dataset"""
          """column_name"""
          """key"""
          """cast(value as double) as value"""
        ]
      }
      {
        operator = REPARTITION
        partitions = 1
      }
    ]
    output = {
      path = "/Users/manhtran/Documents/code/power/spark/src/test/resources/output/desc"
      format = "csv"
      mode = "overwrite"
      options = [
        {key = "header", value = true}
      ]
    }
  }
}